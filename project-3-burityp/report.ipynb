{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W205 Project 3\n",
    "## Priscilla Burity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "\n",
    "In this project we have a mobile app game (a Flask application) in which the user can interact to send to the server the following requests: (i) purchase a sword and (ii) join the gild. Each interaction is an event.\n",
    "\n",
    "Each event triggers an API call to the web server, and the API server then handles that request. \n",
    "\n",
    "We are interested in the API server sending us these events, meaning sending them into the Kafka pipeline. \n",
    "\n",
    "From there we use Spark streaming to filter/select data from Kafka as Kafka is hit by these events. We can do some initial analysis and even transform the events. We also want to pull metadata characteristics of such events, which are simply more information besides the event itself. \n",
    "\n",
    "Next we bring the data into a format that we can then be stored into HDFSS/parquet, which we use as our storage file system.\n",
    "\n",
    "We then use Presto to query stuff back out from HDFS. In order to use Presto, we need the Hive meta store, which is used here as a table names/schema registry.\n",
    "\n",
    "Finally, we sould produce an analytics report to present our queries and findings.\n",
    "\n",
    "To summarize, **Project 3 pipeline consists of the followins steps**:\n",
    "\n",
    "**Step 1** - Spin up the cluster and create a topic on kafka\n",
    "\n",
    "**Step 2** - Built a mobile app game and send requests into the Kafka pipeline\n",
    "\n",
    "**Step 3** - Use Apache Bench to generate test data\n",
    "\n",
    "**Step 4** - Extract events from kafka and write them to HDFS (using Spark streaming)\n",
    "\n",
    "**Step 5** - Use Presto to query data back out from HDFS & show basic analysis of the events\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files\n",
    "\n",
    "Details on the content of these files will be discussed below. But basically, these are the files' names and a short description of their role in the pipeline.  \n",
    "\n",
    "**docker-compose.yml** - Tool for defining and running multi-container Docker applications (as it was part of Project 2, I won't go into details of this file)\n",
    "\n",
    "**game_api.py** - Our Flask application\n",
    "\n",
    "**ab.sh** - Shell script with the the mock up requests so we can generate data.\n",
    "\n",
    "**spark.py** - Script that (1) extract events from kafka, (2) filter, and (3) write them to hdfs. The script also (4) defines our own schema and (5) registers the schema in Hive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands flow\n",
    "\n",
    "We start moving to the project's directory.\n",
    "\n",
    "````\n",
    "cd ~/w205/project-3-burityp\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1 - Spin up the cluster and create a topic on kafka** \n",
    "\n",
    "`docker-compose up` starts the containers. `-d` for detached mode, i.e., starts the containers in the background.\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a topic in kafka. `exec` so I can run arbitrary commands in the services; `kafka` because my topic lives in kafka, then `kafka-topics` to `create` a `topic` that is named `events`, with the usual options (`partitions 1`, `replication-factor 1`). Finally, I set the connection to Zookeeper with the appropriate port number. \n",
    "\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2 - Set a mobile app game and send requests into the Kafka pipeline**\n",
    "\n",
    "Our Flask application is the following:\n",
    "\n",
    "```\n",
    "vim game_api.py\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def default_response():\n",
    "    default_event = {'event_type': 'default'}\n",
    "    log_to_kafka('events', default_event)\n",
    "    return \"This is the default response!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/purchase_a_sword\")\n",
    "def purchase_a_sword():\n",
    "    purchase_sword_event = {'event_type': 'purchase_sword'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"You Purchased a Sword!\\n\"\n",
    "\n",
    "\n",
    "@app.route(\"/join_the_guild\")\n",
    "def join_the_guild():\n",
    "    join_the_guild_event = {'event_type': 'join_guild'}\n",
    "    log_to_kafka('events', join_the_guild_event)\n",
    "    return \"You Joined the Guild!\\n\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this API, we define `producer` to conect to kafka and add `request headers` in the `log_to_kafka` function, so we have the full information concerning that event. This allows us to get some better information about, for example, what browser is used and the source of the host (where did that request come from). \n",
    "\n",
    "We then add the three interactions the user can make in the game, namely `default`, `purchase_a_sword` and `join_guild`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run it in the `mids` container. \n",
    "\n",
    "```\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-burityp/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    " * Serving Flask app \"game_api\"\n",
    " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
    "````\n",
    "\n",
    "This means that the server is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in another (second) terminal we listen to Kafka to we see what hits Kafka.\n",
    "\n",
    "We run kafkacat in the consumer mode (`-C`), the bootstrap servers is `kafka:29092`, we will listen to topic (`-t`) `events`, opening (`-o`) at the `beginning` and without `-e` (end option) so it will run continuously. \n",
    "\n",
    "```\n",
    "docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 - Use Apache Bench to generate test data**\n",
    "\n",
    "Now we need to create stream data. To do so, in a third terminal we will use Apache Bench, which is a tool for benchmarking our HTTP server. It is designed to give an impression of how our Apache installation performs. It can also be used to mock up data (based on generated events) by using `-h` - and that's what we will be doing.\n",
    "\n",
    "We are using a shell script named `ab.sh` with the mock up data. As the game API is running in the first terminal and we are listening to kafka in the second terminal, to check the content of `ab.sh`, we open a third terminal and type:\n",
    "\n",
    "```\n",
    "vim ab.sh\n",
    "```\n",
    "Output:\n",
    "\n",
    "```\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "while true\n",
    "do\n",
    "docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 20 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "docker-compose exec mids ab -n 30 -H \"Host: user1.comcast.com\" http://localhost:5000/join_the_guild\n",
    "\n",
    "docker-compose exec mids ab -n 15 -H \"Host: user2.att.com\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 25 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\n",
    "docker-compose exec mids ab -n 35 -H \"Host: user2.att.com\" http://localhost:5000/join_the_guild\n",
    "\n",
    "docker-compose exec mids ab -n 20 -H \"Host: user3.gogov.com\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 30 -H \"Host: user3.gogov.com\" http://localhost:5000/purchase_a_sword\n",
    "docker-compose exec mids ab -n 40 -H \"Host: user3.gogov.com\" http://localhost:5000/join_the_guild\n",
    "\n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "Here we have three hosts (`user1.comcast.com`, `user2.att.com` and `user3.gogov.com`). \n",
    "\n",
    "Each one of them send the three types of requests `http://localhost:5000/`, `http://localhost:5000/purchase_a_sword` and `http://localhost:5000/join_the_guild`.\n",
    "\n",
    "In each loop the requests are sent *n* times (*n* being a number from 10 to 40 right after `-n` in each line). The `while true` line at the beginning makes the requests to be sent continously until we stop it (by typing `Ctrl C`). As we will use Spark streaming to filter/select data from Kafka as Kafka is hit by the events, this design is useful here as we can test if the stored data is actually growing.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the script\n",
    "\n",
    "```\n",
    "bash ab.sh\n",
    "```\n",
    "\n",
    "As output we get a set of information about our benchmarking run. \n",
    "\n",
    "Recall that the game is runnig in the first terminal and the second terminal is listening to Kafka. Now that we are actually sending the requests via `ab.sh`, the first terminal is now displaying the requests,  while the second terminal is displaying the flow of messages that are hitting Kafka with all the meta information we required in the game API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 - Extract events from kafka and write them to hdfs (using spark streaming)**\n",
    "\n",
    "The next step is to read the data, filter it so we can get the events we are intested in, create the tables and finally write/store them to HDFS. \n",
    "\n",
    "That's what the script `spark.py` does.\n",
    "\n",
    "````\n",
    "vim spark.py\n",
    "````\n",
    "\n",
    "Output:\n",
    "\n",
    "````\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Extract events from kafka and write them to hdfs\n",
    "\"\"\"\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "# Define schema for purchase_sword_event\n",
    "\n",
    "def purchase_sword_event_schema():\n",
    "    \n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "\n",
    "    ])\n",
    "\n",
    "# Define schema for join_guild_event\n",
    "\n",
    "def join_guild_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create a filter for event_type purchase_sword\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'purchase_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Create a filter for event_type join_guild\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start a Spark session\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read from kafka using Spark stream\n",
    "    \n",
    "    raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()\n",
    "\n",
    "    # Filter event 'purchase_sword' from raw_events, register schema in Hive and write the table in HDFS\n",
    "    \n",
    "        # Filter/prepare data\n",
    "    \n",
    "    sword_purchases = raw_events \\\n",
    "        .filter(is_purchase(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          purchase_sword_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "    \n",
    "        # register schema in Hive \n",
    "    \n",
    "    spark.sql(\"drop table if exists sword_purchases\")\n",
    "    sql_string = \"\"\"\n",
    "        create external table if not exists sword_purchases (\n",
    "            raw_event string,\n",
    "            timestamp string,\n",
    "            Accept string,\n",
    "            Host string,\n",
    "            `User-Agent` string,\n",
    "            event_type string\n",
    "            )\n",
    "            stored as parquet\n",
    "            location '/tmp/sword_purchases'\n",
    "            tblproperties (\"parquet.compress\"=\"SNAPPY\")\n",
    "            \"\"\"\n",
    "    \n",
    "    spark.sql(sql_string)\n",
    "    \n",
    "    # write in HDFS\n",
    "    \n",
    "    sink = sword_purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_sword_purchases\") \\\n",
    "        .option(\"path\", \"/tmp/sword_purchases\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "   \n",
    "    \n",
    "    # Filter event 'join_guild' from raw_events, register schema in Hive and write the table in HDFS\n",
    "        \n",
    "        # Filter/prepare data\n",
    "    \n",
    "    join_guilds = raw_events \\\n",
    "        .filter(is_join(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          join_guild_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "    \n",
    "        # register schema in Hive \n",
    "    \n",
    "    spark.sql(\"drop table if exists join_guilds\")\n",
    "    sql_string2 = \"\"\"\n",
    "        create external table if not exists join_guilds (\n",
    "            raw_event string,\n",
    "            timestamp string,\n",
    "            Accept string,\n",
    "            Host string,\n",
    "            `User-Agent` string,\n",
    "            event_type string\n",
    "            )\n",
    "            stored as parquet\n",
    "            location '/tmp/join_guilds'\n",
    "            tblproperties (\"parquet.compress\"=\"SNAPPY\")\n",
    "            \"\"\"\n",
    "    spark.sql(sql_string2)\n",
    "    \n",
    "    # write in HDFS\n",
    "    \n",
    "    sink2 = sword_purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_join_guilds\") \\\n",
    "        .option(\"path\", \"/tmp/join_guilds\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "\n",
    "    spark.streams.awaitAnyTermination()\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "````\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spark.py` file starts importing the necessary libraries as usual. \n",
    "\n",
    "Next we define the schema for the events. This step is not necessary in the current situation as our events are beign generated in a simulated enviroment, but in real life it's useful. This is because the schema is inferred through the first message that hits Kafka and all events coming after that are assumed to have the same schema. So if the first message has the wrong schema, pieces of information of the following messages that don't fit in the schema of the first message will be lost. Defining our own schema is the safest way to make sure that we are not losing relevant information. \n",
    "\n",
    "Next in the script we see the creation of two filters (`is_purchase` and `is_join`) that filter `event_type`s `purchase_sword` and `join_gild` respectively. This will be useful below to write in HDFS different tables for different `event_type`s.\n",
    "\n",
    "Next in the script, we define `spark` for the spark section, then `raw_events` to read from Kafka. In the latter, the `.readStream` option sets Spark streaming to filter/select data from Kafka as Kafka is being hit by the events. The other options here are the usual ones: define the correct boostrap server, subscribe to topic `events`, and read from earliest to latest piece of data. \n",
    "\n",
    "Then for each `event_type` we filter the data in `raw_events` by event type, we pull apart the `timestamp` and add the columns of `value` (with alias `raw_events`). We then use the data and the schema definition set above to create our table. \n",
    "\n",
    "In order to use Presto (step 5 below), we need the Hive meta store, which is used here as table names/schema registry. More details on that will be discussed in Step 5. For now we only need to register our schema in Hive.  \n",
    "\n",
    "To do so, we start by droping the table if it exists, then create the table and specify the schema. We tell Hive it is stored as parquet, give it the location where it's stored in HDFS and how it's compressed. At the end of this block we use `spark.sql` to register that table with Hive.\n",
    "\n",
    "Finally, after all that, we write the tables in HDFS (our `sink`). For stream writing we have to create a `sink` for each destination we want to create. We add `.writeStream` to write in stream mode, in format `parquet` as usual. We add checkpoints that helps keeping the distributed system and the sink in sync. We then add the location where we want to write the tables in HDFS (`.option(\"path\", \"/tmp/...\")`) and set a trigger processing time of 10 seconds. \n",
    "\n",
    "This whole process will be continously running until we terminate it. So we add `spark.streams.awaitAnyTermination()` as the sink is waiting for termination.   \n",
    "\n",
    "Hopefully, we will be able to find those tables in HDFS folders `/tmp/sword_purchases` and `/tmp/join_guilds` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in a fourth terminal, we run it:\n",
    "    \n",
    "```\n",
    "docker-compose exec spark spark-submit /w205/project-3-burityp/spark.py\n",
    "```\n",
    "It will be running, waiting termination. Thus in a fifth terminal we check if the data folders are in HDFS, recalling that Hadoop lives in Cloudera:\n",
    "\n",
    "````\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp\n",
    "````\n",
    "\n",
    "Output:\n",
    "\n",
    "````\n",
    "Found 7 items\n",
    "drwxrwxrwt   - root   supergroup          0 2020-12-05 16:54 /tmp/checkpoints_for_join_guilds\n",
    "drwxrwxrwt   - root   supergroup          0 2020-12-05 16:54 /tmp/checkpoints_for_sword_purchases\n",
    "drwxrwxrwt   - mapred mapred              0 2016-04-06 02:26 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - hive   supergroup          0 2020-12-05 16:54 /tmp/hive\n",
    "drwxrwxrwt   - root   supergroup          0 2020-12-05 16:55 /tmp/join_guilds\n",
    "drwxrwxrwt   - mapred hadoop              0 2016-04-06 02:28 /tmp/logs\n",
    "drwxrwxrwt   - root   supergroup          0 2020-12-05 16:55 /tmp/sword_purchases\n",
    "\n",
    "````\n",
    "\n",
    "Now we check each of the folders `/tmp/sword_purchases` and `/tmp/join_guilds`.\n",
    "\n",
    "````\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases\n",
    "````\n",
    "Output:\n",
    "\n",
    "````\n",
    "Found 13 items\n",
    "drwxr-xr-x   - root supergroup          0 2020-12-05 16:56 /tmp/sword_purchases/_spark_metadata\n",
    "-rw-r--r--   1 root supergroup       3509 2020-12-05 16:56 /tmp/sword_purchases/part-00000-00c74e49-2259-49cd-bcb9-8d7f47d45f90-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2737 2020-12-05 16:55 /tmp/sword_purchases/part-00000-3be93744-539e-48ab-ae8e-a87dae01abf6-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3497 2020-12-05 16:56 /tmp/sword_purchases/part-00000-3f383361-0938-42e7-b928-78cbd74a7ee0-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2787 2020-12-05 16:55 /tmp/sword_purchases/part-00000-496522d1-33bb-4c36-a72e-5965643296d3-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3122 2020-12-05 16:55 /tmp/sword_purchases/part-00000-546f15d8-af8c-433e-be07-35af749d9e00-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3553 2020-12-05 16:56 /tmp/sword_purchases/part-00000-5a771f99-0056-4c3d-8145-013e4bd2d5eb-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3966 2020-12-05 16:55 /tmp/sword_purchases/part-00000-60a93447-0f93-4cb6-b578-c2f0d62fad4e-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2868 2020-12-05 16:56 /tmp/sword_purchases/part-00000-83a58f8e-ca15-4eb2-8ce4-3464df10b75e-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3533 2020-12-05 16:55 /tmp/sword_purchases/part-00000-85d0b252-c283-4196-878e-5f11c094ff22-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3229 2020-12-05 16:55 /tmp/sword_purchases/part-00000-93a694ea-7e4d-433f-945b-9b6eafaa12ca-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       3511 2020-12-05 16:56 /tmp/sword_purchases/part-00000-a1fa6627-588c-40f7-818b-b9e0c2f855aa-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       2880 2020-12-05 16:55 /tmp/sword_purchases/part-00000-c2b71b37-72f4-44a8-982f-386463a5cf31-c000.snappy.parquet````\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "````\n",
    "\n",
    "The output of `docker-compose exec cloudera hadoop fs -ls /tmp/join_guilds` is similar.\n",
    "\n",
    "So everything seems to be ok! So our data has landed in HDFS, which makes us sure that we can read it back out. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5 - Use Presto to query data back out from HDFS & report results**\n",
    "\n",
    "Now it's time to use Presto to query stuff back out from HDFS. \n",
    "\n",
    "As mentioned above, Presto needs the Hive meta store to query from HDFS. In fact, Presto connects to Hive to look up the table names and the meta information and then connects to Hadoop file system to actually read the data so we can send SQL statements.\n",
    "\n",
    "We have already included Hive in our spark script.\n",
    "\n",
    "In `spark.py`, we enabled Hive support in the spark section (`.enableHiveSupport()`) and registered an external table for each event type.\n",
    "\n",
    "Now in order to get to Presto, we run\n",
    "\n",
    "```\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```\n",
    "\n",
    "This leads us to the the Presto console, which we'll use to check some basic features of our tables and report them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please see below some query results**:\n",
    "\n",
    "1 - There are 2 tables in the dataset\n",
    "\n",
    "````\n",
    "presto:default> show tables;\n",
    "\n",
    "      Table      \n",
    "-----------------\n",
    " join_guilds     \n",
    " sword_purchases \n",
    "(2 rows)\n",
    "\n",
    "Query 20201205_170551_00002_dbvdj, FINISHED, 1 node\n",
    "Splits: 2 total, 1 done (50.00%)\n",
    "0:02 [2 rows, 68B] [0 rows/s, 31B/s]\n",
    "\n",
    "````\n",
    "\n",
    "2 - The tables have the schema below (the same for both types of event): \n",
    "\n",
    "```\n",
    "presto:default> describe sword_purchases;\n",
    "   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " raw_event  | varchar |         \n",
    " timestamp  | varchar |         \n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user-agent | varchar |         \n",
    " event_type | varchar |         \n",
    "(6 rows)\n",
    "\n",
    "Query 20201205_170637_00003_dbvdj, FINISHED, 1 node\n",
    "Splits: 2 total, 1 done (50.00%)\n",
    "0:04 [0 rows, 0B] [0 rows/s, 0B/s]\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "presto:default> describe join_guilds;\n",
    "   Column   |  Type   | Comment \n",
    "------------+---------+---------\n",
    " raw_event  | varchar |         \n",
    " timestamp  | varchar |         \n",
    " accept     | varchar |         \n",
    " host       | varchar |         \n",
    " user-agent | varchar |         \n",
    " event_type | varchar |         \n",
    "(6 rows)\n",
    "\n",
    "Query 20201205_170730_00004_dbvdj, FINISHED, 1 node\n",
    "Splits: 2 total, 1 done (50.00%)\n",
    "0:02 [6 rows, 422B] [2 rows/s, 179B/s]\n",
    "```\n",
    "\n",
    "3 - The tables are growing as new data is coming in.\n",
    "\n",
    "We run `SELECT COUNT(*) FROM sword_purchases;` twice so we can see that the tables are growing.\n",
    "\n",
    "1st time (4891 lines):\n",
    "````\n",
    "presto:default> SELECT COUNT(*) FROM sword_purchases;\n",
    " _col0 \n",
    "-------\n",
    "  4891 \n",
    "(1 row)\n",
    "\n",
    "Query 20201205_170832_00005_dbvdj, FINISHED, 1 node\n",
    "Splits: 84 total, 80 done (95.24%)\n",
    "0:23 [4.7K rows, 255KB] [204 rows/s, 11.1KB/s]\n",
    "````\n",
    "\n",
    "2nd time (5096 lines):\n",
    "````\n",
    "presto:default> SELECT COUNT(*) FROM sword_purchases;\n",
    " _col0 \n",
    "-------\n",
    "  5096 \n",
    "(1 row)\n",
    "\n",
    "Query 20201205_171044_00006_dbvdj, FINISHED, 1 node\n",
    "Splits: 96 total, 88 done (91.67%)\n",
    "0:07 [4.83K rows, 274KB] [716 rows/s, 40.6KB/s]\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "3 -  At a given (later) moment 3 hosts sent a different number of requests for the sword_purchase event. For example, user2.att.com sent 1725 requests of this type.\n",
    "\n",
    "````\n",
    "presto:default> SELECT host , COUNT(*) FROM sword_purchases GROUP BY host;\n",
    "       host        | _col1 \n",
    "-------------------+-------\n",
    " user2.att.com     |  1725 \n",
    " user3.gogov.com   |  2070 \n",
    " user1.comcast.com |  1376 \n",
    "(3 rows)\n",
    "\n",
    "Query 20201205_171157_00007_dbvdj, FINISHED, 1 node\n",
    "Splits: 104 total, 102 done (98.08%)\n",
    "0:16 [5.17K rows, 300KB] [329 rows/s, 19.1KB/s]\n",
    "\n",
    "````\n",
    "\n",
    "4 -  Likewise, at a given moment 3 hosts sent a different number of requests for the join_guild event. For example, user3.gogov.com sent 2160 requests of this type.\n",
    "\n",
    "````\n",
    "presto:default> SELECT host , COUNT(*) FROM join_guilds GROUP BY host;\n",
    "       host        | _col1 \n",
    "-------------------+-------\n",
    " user2.att.com     |  1825 \n",
    " user3.gogov.com   |  2160 \n",
    " user1.comcast.com |  1457 \n",
    "(3 rows)\n",
    "\n",
    "Query 20201205_171730_00008_dbvdj, FINISHED, 1 node\n",
    "Splits: 129 total, 127 done (98.45%)\n",
    "0:25 [5.37K rows, 324KB] [214 rows/s, 13KB/s]\n",
    "\n",
    "````\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the project :)\n",
    "    \n",
    "Thank you!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
