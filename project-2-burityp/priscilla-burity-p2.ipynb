{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W205 Project 2\n",
    "## Priscilla Burity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to the project's directory.\n",
    "\n",
    "````\n",
    "cd ~/w205/project-2-burityp\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the .yml file from class. `cp` for copy and `-r` to copy recursively, so `cp` copies the contents of directories, and if a directory has subdirectories they are copied (recursively) too.\n",
    "\n",
    "```\n",
    "cp -r ~/w205/spark-with-kafka-and-hdfs/docker-compose.yml ~/w205/project-2-burityp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the .yml file using the `vim` command.\n",
    "\n",
    "`vim docker-compose.yml`\n",
    "\n",
    "The file lists the following servers: `kafka` because I want to publish/consume messages with Kafka;   `zookeeper`,  which keeps track of status of the Kafka cluster nodes, topics, partitions etc; `spark` because I will transform data in Spark. Spark depends on `cloudera` (also in the .yml file), which supports Haddop file system, where I will store our results. Finally, my `docker-compose.yml` file has the course (`mids`) container. The question statement doesn't ask to dig into details of the .yml file, so I won't.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, spin-up the cluster. `docker-compose up` starts the containers. `-d` for detached mode, i.e., to start the containers in the background.\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, `docker-compose logs` to attach to the logs of the running services. Check if there are any error messages. `-f` means I follow the log. `kafka` because I'm interested in logs of this specific container.\n",
    "\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the error message: `There is insufficient memory for the Java Runtime Environment to continue.`\n",
    "\n",
    "Increase the swap space of the hardisk:\n",
    "\n",
    "```\n",
    "sudo dd if=/dev/zero of=/var/myswap bs=1M count=2048\n",
    "\n",
    "sudo mkswap /var/myswap\n",
    "\n",
    "sudo swapon /var/myswap\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retype the commands and check if there are any error messages.\n",
    "\n",
    "````\n",
    "docker-compose up -d\n",
    "\n",
    "docker-compose logs -f kafka\n",
    "\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds ok. Download the data\n",
    "````\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out Hadoop file system. In the line below, I use `cloudera` to talk to `hadoop`, `fs` for file system. `-ls` to check the content of the temporary (`/tmp`) directory. \n",
    "\n",
    "````\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "````\n",
    "\n",
    "Check if the automatically created folders are there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds ok. Now I need to create a topic in kafka. In the line below, I type `docker-compose exec` so I can run arbitrary commands in the services; `kafka` because my topic lives in kafka, then `kafka-topics` to `create` a `topic` that is named `assessments`, with the usual options (`partitions 1`, `replication-factor 1`). Finally, I set the connection to Zookeeper with the appropriate port number. \n",
    "\n",
    "````\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I use kafkacat to produce messages to the `assessments` topic. \n",
    "\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-burityp/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "\n",
    "Use pipes `|` - pipe takes what previous thing gave to it and passes it on. \n",
    "\n",
    "`docker-compose exec` so I can run arbitrary commands in the services, `mids` because I'm in the mids container, `bash` to implement the shell in the container, `-c` so the commands are read from string right after `-c` (i.e., inside the parenthesis). `cat` to show a file in the path `/w205/project-2-burityp/assessment-attempts-20180128-121051-nested.json`, `jq` to pretty-print it, `'.[]'` to return each element of the array returned in the response, one at a time, `-c` to show it inline.\n",
    "\n",
    "`kafkacat` allows us to produce, consume, and list topic and partition information for Kafka. In the current case, I'm producing messages with kafka (thus `-P`). I should also supply kafkacat with a broker (`-b kafka:29092`) and a topic (`-t assessments`).   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, spin up a `pyspark` process using the `spark` container.\n",
    "\n",
    "````\n",
    "docker-compose exec spark pyspark\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to read in the assessments data. So at the pyspark prompt, I type:\n",
    "\n",
    "````\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load()\n",
    "````\n",
    "\n",
    "Here I `spark.read` a dataset with the `format` `kafka` and name it as `raw_assessments`. As I want to read from Kafka, I need a `bootstrap.server` to connect to the `kafka` server with the appropriate port number. Then I `subscribe` to one topic (`assessments`) and set the `startingOffsets` and `endingOffsets` as `earliest` and `latest` respectively to have the whole dataset. And then `load` the data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cache` this to keep in memory and cut back on warnings later.\n",
    "\n",
    "```\n",
    "raw_assessments.cache()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `Schema` of the data.\n",
    "\n",
    "````\n",
    "raw_assessments.printSchema()\n",
    "````\n",
    "I see:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select` column `value` in `raw_assessments` and make it a `string`.\n",
    "\n",
    "````\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `assessments` data and confirms it is a column of jason lines.\n",
    "\n",
    "````\n",
    "assessments.show()\n",
    "\n",
    "````\n",
    "I see:\n",
    "```\n",
    "+--------------------+\n",
    "|               value|\n",
    "+--------------------+\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "+--------------------+\n",
    "```\n",
    "This is not a helpful format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the data, I have to implement the `sys` library, which gives access to certain UNIX system methods/tools.\n",
    "\n",
    "````\n",
    "import sys\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `encoding` to `utf8` and make sure that we can write and deal with the data.\n",
    "\n",
    "````\n",
    "sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf8', buffering=1)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import json` to be able to deal with json format \n",
    "\n",
    "````\n",
    "import json\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `Row` from `pyspark.sql` to access data by rows.\n",
    "\n",
    "````\n",
    "from pyspark.sql import Row\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I extract the data.  \n",
    "\n",
    "```\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_assessments.show()\n",
    "````\n",
    "\n",
    "The first line above does the following: from the dataset`assessments`, I get `rdd` (spark's distributed dataset) and apply a `map` to it. `rdd.map` is a transformation that passes each dataset element through a function (in the current case, the`lamba` expression that `loads` the `json` file by `Row`) and returns a new RDD representing the results. Then I turn it into a data frame (the `toDF` part). In the second line above, I ask to `show` it. \n",
    "\n",
    "I see:\n",
    "\n",
    "````\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
    "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
    "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
    "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
    "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
    "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
    "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
    "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
    "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
    "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
    "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
    "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "only showing top 20 rows\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema.\n",
    "\n",
    "```\n",
    "extracted_assessments.printSchema()\n",
    "```\n",
    "Now I see:\n",
    "\n",
    "````\n",
    "root\n",
    " |-- base_exam_id: string (nullable = true)\n",
    " |-- certification: string (nullable = true)\n",
    " |-- exam_name: string (nullable = true)\n",
    " |-- keen_created_at: string (nullable = true)\n",
    " |-- keen_id: string (nullable = true)\n",
    " |-- keen_timestamp: string (nullable = true)\n",
    " |-- max_attempts: string (nullable = true)\n",
    " |-- sequences: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: array (valueContainsNull = true)\n",
    " |    |    |-- element: map (containsNull = true)\n",
    " |    |    |    |-- key: string\n",
    " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
    " |-- started_at: string (nullable = true)\n",
    " |-- user_exam_id: string (nullable = true)\n",
    "````\n",
    "\n",
    "There is a nested field in `sequences` that I might have to unwrap, depending on the variables under analysis.\n",
    "\n",
    "Now I'm able to answer some questions! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - How many assesstments are in the dataset? \n",
    "\n",
    "Answer: 3280\n",
    "\n",
    "Code: `assessments.count()` counts the number of entries in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - What's the name of your Kafka topic? How did you come up with that name?\n",
    "\n",
    "Answer: `assessments`. It seemed natural as it is an assessments dataset. If it were a real corporate enviroment though I would use a naming convention such as `<message type>.<dataset name>.<data name> `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - How many **people** took Learning Git?\n",
    "\n",
    "In the dataset, I do not have a course data, only the exam data (`exam_name`) and the user id (`user_exam_id`). But some users took the same exam more than once. How do I know that?\n",
    "\n",
    "By typing \n",
    "\n",
    "```\n",
    "extracted_assessments.groupby('exam_name','user_exam_id').count().sort(['count'], ascending=False).show(40)\n",
    "```\n",
    "\n",
    "In the line above I `count` the frequencies of (`exam_name`,`user_exam_id`) grouped together and show the results table in descending order (limited to the first `40` entries to test if it's enough). The output was the following:\n",
    "\n",
    "````\n",
    "+--------------------+--------------------+-----+                               \n",
    "|           exam_name|        user_exam_id|count|\n",
    "+--------------------+--------------------+-----+\n",
    "|        Learning Git|a7e6fc04-245f-4e3...|    3|\n",
    "|Beginning C# Prog...|6132da16-2c0c-436...|    3|\n",
    "|Learning C# Desig...|fa23b287-0d0a-468...|    3|\n",
    "|        Learning DNS|bd96cfbe-1532-4ba...|    3|\n",
    "|Beginning C# Prog...|00745aef-f3af-412...|    3|\n",
    "|Learning C# Desig...|1e325cc1-47a9-480...|    3|\n",
    "|Beginning C# Prog...|37cf5b0c-4807-421...|    3|\n",
    "|Intermediate C# P...|949aa36c-74c7-4fc...|    3|\n",
    "|Learning C# Best ...|3d63ec69-8d97-4f9...|    3|\n",
    "|Learning C# Best ...|a45b5ee6-a4ed-4b1...|    3|\n",
    "|An Introduction t...|d4ab4aeb-1368-486...|    3|\n",
    "|Intermediate C# P...|028ad26f-a89f-4a6...|    3|\n",
    "|Beginning C# Prog...|a244c11a-d890-4e3...|    3|\n",
    "|Introduction to B...|b7ac6d15-97e1-4e9...|    3|\n",
    "|Beginning C# Prog...|66d91177-c436-4ee...|    3|\n",
    "|Learning C# Best ...|ac80a11a-2e79-40e...|    3|\n",
    "|Beginning C# Prog...|c320d47f-60d4-49a...|    3|\n",
    "|        Learning Git|cdc5859d-b332-4fb...|    3|\n",
    "|Intermediate Pyth...|6e4889ab-5978-44b...|    2|\n",
    "|Intermediate Pyth...|c1eb4d4a-d6ef-43e...|    2|\n",
    "|Learning to Progr...|f099f716-1e3b-4c3...|    1|\n",
    "|Python Data Struc...|c50bc296-6a58-425...|    1|\n",
    "|       Mastering Git|89d8acd5-d1d6-402...|    1|\n",
    "|Introduction to B...|1e272721-f5ce-45d...|    1|\n",
    "|        Learning Git|2f9aeb8b-0e47-47f...|    1|\n",
    "|Introduction to P...|351e22d2-3756-449...|    1|\n",
    "|Being a Better In...|bf6f1140-476b-4b6...|    1|\n",
    "|Beginning Program...|d8ec4756-9d94-416...|    1|\n",
    "|        Learning Git|8721df35-0f77-41f...|    1|\n",
    "|Intermediate Pyth...|fe60be01-4cde-495...|    1|\n",
    "|        Learning Git|dcc916fd-405e-43f...|    1|\n",
    "|        Learning Git|500a4188-0be0-43b...|    1|\n",
    "|Data Science with...|50dfd4a4-1955-4af...|    1|\n",
    "|Cloud Native Arch...|bb6fb68e-c640-4a5...|    1|\n",
    "|JavaScript: The G...|af7c91db-c7a3-4b9...|    1|\n",
    "|        Learning Git|eea60cd3-24c7-403...|    1|\n",
    "|    Learning Eclipse|9a99067a-d4af-4a2...|    1|\n",
    "|        Learning DNS|89472bb4-0b8f-4cc...|    1|\n",
    "|    Learning Eclipse|32d84623-fe00-48f...|    1|\n",
    "|Introduction to P...|a1b50058-2ae6-49f...|    1|\n",
    "+--------------------+--------------------+-----+\n",
    "only showing top 40 rows\n",
    "````\n",
    "\n",
    "\n",
    "So I found out that some people took the same exam 2 to 3 times. More specifically, 2 users took Learning Git 3 times!\n",
    "\n",
    "So in order to know how many *people* took Learning Git, I have to keep in the dataset only those entries that correspond to people taking the Learning Git exam for the first time.\n",
    "\n",
    "So I created a data frame named `extracted_assessments_nodupl` to drop duplicates of the (`exam_name`,`user_exam_id`) group in `extracted_assessments`.\n",
    "\n",
    "```\n",
    "extracted_assessments_nodupl = extracted_assessments.drop_duplicates(subset =('exam_name','user_exam_id')) \n",
    "```\n",
    "\n",
    "Next I typed\n",
    "\n",
    "```\n",
    "extracted_assessments_nodupl.groupby('exam_name').count().sort(['count'], ascending=False).show(10000, False)\n",
    "````\n",
    "\n",
    "This line does the following: group the `extracted_assessments_nodupl` data frame by the `exam_name` column, `count` the frequencies of each distint value in this column, `sort` in descending order (`ascending=False`), `show` the first 1000 entries (I had already checked that there are fewer distinct values for `exam_name`) and do not cut `exam_name` final strings (`False` inside `show`)\n",
    "\n",
    "The output is the following:\n",
    "\n",
    "````\n",
    "+-----------------------------------------------------------------------+-----+ \n",
    "|exam_name                                                              |count|\n",
    "+-----------------------------------------------------------------------+-----+\n",
    "|Learning Git                                                           |390  |\n",
    "|Introduction to Python                                                 |162  |\n",
    "|Introduction to Java 8                                                 |158  |\n",
    "|Intermediate Python Programming                                        |156  |\n",
    "|Learning to Program with R                                             |128  |\n",
    "|Introduction to Machine Learning                                       |119  |\n",
    "|Software Architecture Fundamentals Understanding the Basics            |109  |\n",
    "|Learning Eclipse                                                       |85   |\n",
    "|Beginning C# Programming                                               |83   |\n",
    "|Learning Apache Maven                                                  |80   |\n",
    "|Beginning Programming with JavaScript                                  |79   |\n",
    "|Mastering Git                                                          |77   |\n",
    "|Introduction to Big Data                                               |73   |\n",
    "|Advanced Machine Learning                                              |67   |\n",
    "|Learning Linux System Administration                                   |59   |\n",
    "|JavaScript: The Good Parts Master Class with Douglas Crockford         |58   |\n",
    "|Learning SQL                                                           |57   |\n",
    "|Practical Java Programming                                             |53   |\n",
    "|HTML5 The Basics                                                       |52   |\n",
    "|Python Epiphanies                                                      |51   |\n",
    "|Software Architecture Fundamentals Beyond The Basics                   |48   |\n",
    "|Introduction to Data Science with R                                    |43   |\n",
    "|Intermediate C# Programming                                            |39   |\n",
    "|Learning DNS                                                           |38   |\n",
    "|Expert Data Wrangling with R                                           |35   |\n",
    "|Mastering Advanced Git                                                 |34   |\n",
    "|An Introduction to d3.js: From Scattered to Scatterplot                |31   |\n",
    "|Data Visualization in R with ggplot2                                   |31   |\n",
    "|Python Data Structures                                                 |29   |\n",
    "|Learning C# Best Practices                                             |29   |\n",
    "|Cloud Native Architecture Fundamentals                                 |29   |\n",
    "|Introduction to Time Series with Team Apache                           |28   |\n",
    "|Git Fundamentals for Web Developers                                    |28   |\n",
    "|Introduction to Shiny                                                  |27   |\n",
    "|Learning Linux Security                                                |27   |\n",
    "|Learning Java EE 7                                                     |25   |\n",
    "|Mastering Python - Networking and Security                             |25   |\n",
    "|Using R for Big Data with Spark                                        |24   |\n",
    "|TCP/IP                                                                 |21   |\n",
    "|Reproducible Research and Reports with R Markdown                      |21   |\n",
    "|JavaScript Templating                                                  |21   |\n",
    "|Learning C# Design Patterns                                            |19   |\n",
    "|Refactor a Monolithic Architecture into Microservices                  |17   |\n",
    "|Cloud Computing With AWS                                               |17   |\n",
    "|Learning iPython Notebook                                              |17   |\n",
    "|Learning Apache Hadoop                                                 |16   |\n",
    "|Design Patterns in Java                                                |15   |\n",
    "|Networking for People Who Hate Networking                              |15   |\n",
    "|Relational Theory for Computer Professionals                           |15   |\n",
    "|I'm a Software Architect, Now What?                                    |15   |\n",
    "|Great Bash                                                             |14   |\n",
    "|Introduction to Architecting Amazon Web Services                       |14   |\n",
    "|Working with Algorithms in Python                                      |14   |\n",
    "|Introduction to Apache Kafka                                           |13   |\n",
    "|Offline Web                                                            |13   |\n",
    "|Introduction to Modern Client-Side Programming                         |13   |\n",
    "|Learning Data Structures and Algorithms                                |13   |\n",
    "|Learning Apache Cassandra                                              |12   |\n",
    "|Amazon Web Services - Simple Storage Service                           |12   |\n",
    "|SQL: Beyond the Basics                                                 |11   |\n",
    "|Amazon Web Services - Virtual Private Cloud                            |11   |\n",
    "|Learning SQL for Oracle                                                |11   |\n",
    "|The Principles of Microservices                                        |11   |\n",
    "|Event-Driven Microservices                                             |10   |\n",
    "|Architectural Considerations for Hadoop Applications                   |10   |\n",
    "|Being a Better Introvert                                               |10   |\n",
    "|Arduino Prototyping Basics                                             |10   |\n",
    "|Learning Data Modeling                                                 |9    |\n",
    "|A Practical Introduction to React.js                                   |9    |\n",
    "|Introduction to Apache Spark                                           |9    |\n",
    "|Web & Native Working Together                                          |8    |\n",
    "|Introduction to Hadoop YARN                                            |8    |\n",
    "|Nullology                                                              |8    |\n",
    "|Arduino Inputs                                                         |8    |\n",
    "|Normal Forms and All That Jazz Master Class                            |7    |\n",
    "|Introduction to Apache Hive                                            |7    |\n",
    "|Data Science with Microsoft Azure and R                                |7    |\n",
    "|Collaborating with Git                                                 |6    |\n",
    "|Introduction to Modern Front-End Development                           |6    |\n",
    "|Hadoop Fundamentals for Data Scientists                                |6    |\n",
    "|Introduction to Amazon Web Services (AWS) - EC2 Deployment Fundamentals|6    |\n",
    "|Starting a Grails 3 Project                                            |5    |\n",
    "|Modeling for Software Architects                                       |5    |\n",
    "|An Introduction to Set Theory                                          |5    |\n",
    "|Example Exam For Development and Testing oh yeahsdf                    |5    |\n",
    "|View Updating                                                          |4    |\n",
    "|Using Storytelling to Effectively Communicate Data                     |4    |\n",
    "|Service Based Architectures                                            |3    |\n",
    "|Using Web Components                                                   |3    |\n",
    "|Mastering Web Views                                                    |3    |\n",
    "|Getting Ready for Angular 2                                            |3    |\n",
    "|Building Web Services with Java                                        |3    |\n",
    "|The Closed World Assumption                                            |2    |\n",
    "|Arduino Prototyping Techniques                                         |2    |\n",
    "|Understanding the Grails 3 Domain Model                                |2    |\n",
    "|What's New in JavaScript                                               |2    |\n",
    "|Learning Spring Programming                                            |2    |\n",
    "|Hibernate and JPA Fundamentals                                         |2    |\n",
    "|Client-Side Data Storage for Web Developers                            |2    |\n",
    "|Learning to Visualize Data with D3.js                                  |1    |\n",
    "|Native Web Apps for Android                                            |1    |\n",
    "|Nulls, Three-valued Logic and Missing Information                      |1    |\n",
    "|Operating Red Hat Enterprise Linux Servers                             |1    |\n",
    "+-----------------------------------------------------------------------+-----+\n",
    "````\n",
    "\n",
    "\n",
    "Answer: 390 took Learning Git. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-What is the least common course taken? And the most common?\n",
    "\n",
    "Answer: The least commons (1 each) are: Learning to Visualize Data with D3.js (poor D3!), Native Web Apps for Android, Nulls, Three-valued Logic and Missing Information, Operating Red Hat Enterprise Linux Servers. The most common is Learning Git (390).\n",
    "\n",
    "Code (the same as in question 3): `extracted_assessments_nodupl.groupby('exam_name').count().sort(['count'], ascending=False).show(10000, False)` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the the project pipeline, I have to land the messages transfomed in Spark in HDFS.\n",
    "\n",
    "As data scients will consume our data, I'll land the full transformed dataset (`extracted_assessments`) and not the data frame in which I eliminated users taking the same exam for second and third times (`extracted_assessments_nodupl`). As the nested fields departing from `sequences` are not important for my analys, I did not use them and decided to leave them as they are. \n",
    "\n",
    "Thus in the line below I `write` `extracted_assessments` in `parquet` format in the file in the path `/tmp/extracted_assessments`.\n",
    "\n",
    "````\n",
    "extracted_assessments.write.parquet(\"/tmp/extracted_assessments\")\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I check out the result typing in another terminal (out of pyspark):\n",
    "\n",
    "````\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/\n",
    "\n",
    "````\n",
    "As above, I check out the Hadoop file system, using `cloudera` conteiner to talk to `hadoop`, `fs` for file system. `-ls` to check the content of the temporary (`/tmp`) directory in human readeable format (`-h`).\n",
    "    \n",
    "\n",
    "I should see the `extracted_assignments` folder in there.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I stop the containers by typing:\n",
    "\n",
    "````\n",
    "docker-compose down\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
